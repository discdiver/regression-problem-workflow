{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "scrolled": true,
        "collapsed": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom math import sqrt\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', 300)\n\nfrom sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler, MinMaxScaler, normalize\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,  cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn_pandas import CategoricalImputer\n\nimport os\n#print(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
      },
      "cell_type": "markdown",
      "source": "# Basic Workflow for Machine Learning\n## By Jeff Hale"
    },
    {
      "metadata": {
        "_cell_guid": "69db5f14-c556-4604-b3f4-7adc52bc30bb",
        "_uuid": "617c731be6c85ceb7a7bf11d04c236f3b31462b7"
      },
      "cell_type": "markdown",
      "source": "## Step 1: Write out your goal\n\nStart with the big picture. Why are you interested in looking at some data and making a predictive model? Hoping to figure out how to beat the stock market? Want to predict where Malaria will show up? You've got something in mind. It's your motivating goal.\n\nFor this project we're documenting a machine learning workflow for a regression task. In particular, predicting housing prices with the [Ames Housing Dataset](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf). We want to minimize the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. \n\nWe'll do some data cleaning and feature engineering and then explore a variety of machine learning algorithms. We'll try some OLS regression variations and then try KNN  and decision-tree-based algorithms.  "
    },
    {
      "metadata": {
        "_cell_guid": "6d5d2d4c-2fe4-408b-b558-58a43635dbb4",
        "_uuid": "644faaef3efc741a80eebd67772d2f75ce37faf8"
      },
      "cell_type": "markdown",
      "source": "## Step 2: Explore variable data information\nWrite out the variables with definitions and thoughts in a google sheet. [Here's mine](https://docs.google.com/spreadsheets/d/106ZP2r97yRkkTbBqV9oEt00XNnjomhj3BvIaCNaeWlk/edit?usp=sharing) for this data set. This idea came from [ Pedro Marcelino's popular Kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python).\n\nThis exercise was very valuable. Insights:\n1. There is probably a lot of duplication of information in the sense of very high correlation in the data. We'll probably be selecting a limited number of features to make a better model.\n2. A lot of variables probably don't have much effect on the price of housing. We'll probably be selecting a limited number of features to make a more parsimonious regression model.\n3. A *SaleType* of *COD* is likely to be important because a *Court Officer Deed/Estate* would be expected to substantially reduce the sale price of a property..\n4.  *SaleCondition* categories that aren't *Normal* are likely to have large effects to the price.\n4. A good deal of the data is ordinal.\n"
    },
    {
      "metadata": {
        "_uuid": "cedb9779e9d7804cd5c2e07e708803f369caf4a6"
      },
      "cell_type": "markdown",
      "source": "## Ordinal Data\n\nInterval and ratio data types are made of equally spaced quantities. For example, one cup of milk is exactly twice the amount of two cups of milk. Five cups of milk is exactly five times the amount of one cup of milk. (Ratio data has a meaningful zero point, but both data types can generally be used similarly in machine learning).\n\nOrdinal data is in rank order, but not necessarily evenly spaced. For example, a movie rating of two stars is higher than a movie rating of one star, and a movie rating of five stars is higher than two stars. But we can't say that a movie that has five stars is five times better than a movie with a one star rating.\n\nNominal data doesn't have any clear numeric value scale associated with it. For example, if we are looking at the effect of shirt color on test scores, whether a shirt is blue or red doesn't have a numeric scale associated with being blue or red.\n\nI hadn't previously come across methods for preserving the informational value in ordinal data in machine learning, so I went down the proverbial rabbit hole researching options.\n\nThere isn't a lot of info on the treadtment of ordinal data in machiine learning, which I find surprising because it's so common. Here's what I found\n\n1. One hot encoding the categories is the predominent method for handling ordinal data. \n2. Decision tree based models might find the ordinal relationship naturally, or they might not depending upon their parameter settings. Seems better not to leave it to chance. A bagging algorithm with decision trees, such as a Random Forest Regressor or Randome Forest Classifier should be more likely to find the orindal relationship because it runs many models.\n3. The categories could be treated as interval data, although they are strictly not because the space between each category is not likely to be exactly the same. This is a fairly easy way to go about coding the data and should result in better outcomes than treating the data as categorical.\n4. The categories can be treated as binary data and this might work well in some cases.\n5. There's a very promising approach outlined in a[ 2106 paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12328) by researchers at USC. They have a method for estimating the size of the intervals between the categories. Turning this method into a sklearn package would be a cool project.\n6. [Category_encoder package](http://contrib.scikit-learn.org/categorical-encoding/) makes this process easeir and was used by [one Kaggler ](https://www.kaggle.com/kilaorange/keras-deep-learning-approach)to transform the dataset into binary encoded data. \n7. Will McGinnis is the primary author of the category_encoder package and has a nice [blog](http://www.willmcginnis.com/2016/12/18/basen-encoding-grid-search-category_encoders/) where he explains some of his research evaluating different categorical encoding methods with different data sets. \n\n\nI'm going to treat the ordinal data as intervals for this project because that seems like a reasonable time/result trade off and leave further explorations for other projects.\n\nI plan to make another kernel exploring methods for  transforming categorical data as part of preprocessing steps in a pipeline."
    },
    {
      "metadata": {
        "_cell_guid": "5a3ae4c5-aad2-473c-9cc8-d2acdf7696f1",
        "collapsed": true,
        "_uuid": "aafd90ba74ff9d44b95f51bf45eab49ad5339deb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# load the data\ntrain_data = pd.read_csv(\"../input/train.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5fbc911a-0c6c-4125-ad6c-80af5d871c5e",
        "_uuid": "41deecab69f740da2d4c1aa292c16b40fa91461a"
      },
      "cell_type": "markdown",
      "source": "## Step 3: Preliminarily data investigation\nRun info, describe, and head methods to get a better feel for the data and some basic descriptive statistics."
    },
    {
      "metadata": {
        "_cell_guid": "fcccb945-a63e-4bb1-a3f6-58274933e5ae",
        "_uuid": "b6e3b821bfad84e3dbfa6b7a8f6c63baeffbf850",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(train_data.shape)\nprint(train_data.describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "27b7bbc0-2e14-4a5f-a813-6602699b792f",
        "_uuid": "9a560fb89a8ad7d3e57fbaf43380fe72b018b918",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(train_data.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7fca9f7f-95c0-4f2e-8615-b138f70ea4d6",
        "_uuid": "83c928026424023af7043d1c4009bb12c0d2ce9e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(train_data.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7e55844d-89c9-4bfe-abfd-2e40751e8997",
        "_uuid": "cb06a73a5bfe2c5bbb73acf9735dfe78c308bc67"
      },
      "cell_type": "markdown",
      "source": "## Step 4: Let's make a very basic model with only square feet feature predicting the sale price.\nWe'll drop the null values and then fit a linear regression model."
    },
    {
      "metadata": {
        "_cell_guid": "fb8bdbea-8efb-40f3-80c1-8c5a852a90ec",
        "collapsed": true,
        "_uuid": "396be762c9c70e02aa0e2e9ebb514f3944ecd27b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "sqft = pd.DataFrame(train_data['GrLivArea'].values.reshape(-1,1), columns = ['GrLivArea'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5b84f0a4-d8a9-4554-89c8-e0ddd42c7565",
        "_uuid": "427c946855a4ff43978cfd82fb04cc731ff237ac",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sqft.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fc00a1df-f0f7-4aaa-928e-c0673cb391f9",
        "_uuid": "0183d8c679e3675e950fb5efe305b663fc33e533",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sqft.isnull().any()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2818d440-7d39-46ca-b332-6ed697d6bd0b",
        "_uuid": "253fbe466a4a4f0029ea89630b995b95fb7d09d7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "BX_train, BX_test, by_train, by_test = train_test_split(sqft, train_data['SalePrice'], test_size = .4, random_state = 34)\nblr = LinearRegression()\nblr.fit(BX_train, by_train)\npredictions = blr.predict(BX_test)\n\nrmse = sqrt(mean_squared_error(by_test, predictions))\nprint(\"score = {0:,.0f}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "84d1ee36-d3eb-40da-8aad-24244ceacde3",
        "_uuid": "3fa6d17d9b0977dadd04f0f96a7a117e75f26556"
      },
      "cell_type": "markdown",
      "source": "## Step 5: Let's add some other numeric features and try a few more models\nThat RMSE isn't awful, but it's a bit high. Let's see if we can get it down by adding a few other predictors that are likely to have a large influence on the sale price."
    },
    {
      "metadata": {
        "_cell_guid": "218e25d8-d039-4f94-995e-ca40760a34c3",
        "_uuid": "c163c13d5395ae4e1ac3c0bf559fe88a0b567f00",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "five_cols = ['GrLivArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd']\nfive_features = train_data.loc[:, five_cols]\nprint(five_features.head(3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d4207f55-2538-4366-bfc8-d737da9ad9d0",
        "_uuid": "dcd672ed9b8e4e60f459db54cd2b30004c48a458",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# function to make models, fit, predict with a list of columns as a parameter\n\ndef make_model(cols):\n    X_train, X_test, y_train, y_test = train_test_split(train_data[cols], train_data['SalePrice'], test_size = .4, random_state = 34)\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    predictions = lr.predict(X_test)\n\n    score = sqrt(mean_squared_error(y_test, predictions))\n    print(\"Columns: {0}, score: {1:,.0f}\".format(cols, score))\nmake_model(five_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "42aa8dcd-c1e1-4a35-a731-db53d71af069",
        "_uuid": "83ce927382ad49e81bdf440e2c086950d2773a7f"
      },
      "cell_type": "markdown",
      "source": "RMSE dropped to 45k. Now we're cooking!"
    },
    {
      "metadata": {
        "_cell_guid": "69c45c23-3b7d-4ad3-9bac-f2d9e33d5a4f",
        "_uuid": "0d40cbb2d4511764c93cbb0aed4830da1af83500"
      },
      "cell_type": "markdown",
      "source": "Let's try the same five features with a basic KNN model"
    },
    {
      "metadata": {
        "_cell_guid": "c25912fa-2ccb-47bb-aae2-bf4fa42cc040",
        "_uuid": "13c2b41358b1614126e66bbf0fc89bf7faed4906",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# better function to make models, fit, predict that can take a model type as a parameter\n\ndef make_model(cols, mod_type):\n    X_train, X_test, y_train, y_test = train_test_split(train_data[cols], train_data['SalePrice'], test_size = .4, random_state = 34)\n    mod = mod_type\n    mod.fit(X_train, y_train)\n    predictions = mod.predict(X_test)\n\n    score = sqrt(mean_squared_error(y_test, predictions))\n    print(\"Columns: {0}, score: {1:,.0f}\".format(cols, score))\nmake_model(five_cols, KNeighborsRegressor())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "748c1e5f-db6e-4a5d-9138-e6fba5f5ba97",
        "_uuid": "578f79bac73161247b45e56a7db2e28766da6337"
      },
      "cell_type": "markdown",
      "source": "Not so good with the default KNN parameters.  Let's make a loop to try a bunch of models with their preset parameters."
    },
    {
      "metadata": {
        "_cell_guid": "4c72f416-e5f0-406a-92fa-1568d33c61aa",
        "_uuid": "5e363142a29cf20c386c8473e2f89e070b2d10bf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mods = [Lasso(), Ridge(), GradientBoostingRegressor(), AdaBoostRegressor()]\n\nfor mod in mods:\n    make_model(five_cols, mod)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e75b5e3b-ebc6-4afc-93cb-b715f1367c16",
        "_uuid": "c8ee410ff181c0eb7cf9f0d12d436976ef992d5b"
      },
      "cell_type": "markdown",
      "source": "Lasso is just limiting the number of features, which we already limited, so not seeing an improved score from that. Ridge is about the same score. The boosted regressors are about the same.  I think it's time to add some categorical data, clean some data, transform some data, and feature engineer."
    },
    {
      "metadata": {
        "_cell_guid": "2b76fe73-9e91-4889-9d41-b940b1e94466",
        "_uuid": "b911a13d7b23a3d592074e7fdd5bcc66274e2eaf"
      },
      "cell_type": "markdown",
      "source": "## Step 6: Basic data visualization \nMake a correlation plot, heatmap, etc. to see what you have time to see."
    },
    {
      "metadata": {
        "_cell_guid": "0af38bdc-bea1-461f-9e3a-3fd1b5adc7d6",
        "_uuid": "d9837893af83fe51384d54718bb4876870ea379f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.pairplot(five_features) \nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2ac60cf3-5b9b-47d2-a8ef-01fb26a5cf4d",
        "_uuid": "9d64707c1f664891e2b86432ad2c8b427008c400",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "y_df = train_data['SalePrice'].to_frame()\nfive_features['sale_price'] = y_df\n#print(five_features)\ncorrelation = five_features.corr()\nprint(correlation)\nsns.heatmap(correlation)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7aa64e1d-d142-4110-a896-96612842dada",
        "_uuid": "61f1807c1cf4d66b694d31654bdd8371f2bca8ff"
      },
      "cell_type": "markdown",
      "source": "## Step 7: Add ordinal features\nLet's seperate the column types into numeric, ordinal, and categorical data types."
    },
    {
      "metadata": {
        "_cell_guid": "9f5d9262-0f2b-43df-a728-ae5ac40982c3",
        "_uuid": "93b22d5ffce7c81b12687aecc5dcb1e42a406d8a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# make lists of the numerical and string variable categories\nnumeric_cols = []\nstring_cols = []    # ordinal and categorical\n\n\nfor col in train_data.columns:\n    # True integer or float columns\n    if (train_data.dtypes[col] == np.int64 or train_data.dtypes[col] == np.int32 or train_data.dtypes[col] == np.float64):\n        numeric_cols.append(col)\n    # Categorical and oridnal columns\n    if (train_data.dtypes[col] == np.object):  \n        string_cols.append(col)\n        \nprint(numeric_cols)\nprint(string_cols)\n            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "149bf9ae-ae94-4b21-89e1-49f4c2847cdd",
        "collapsed": true,
        "_uuid": "ae8f244199670909351c8ef74ed4cf7b47dba795",
        "trusted": false
      },
      "cell_type": "code",
      "source": "y = train_data['SalePrice']\nX = train_data.drop(columns = [\"SalePrice\"], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5cc587b4-b929-4dc9-80e7-64a644c9ac51",
        "_uuid": "b94b8373a86317aa979cb65ccd445ab4a8fa2a3c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0b4ced21-630f-44a4-834b-b2b158a0a6bb",
        "collapsed": true,
        "_uuid": "6892b3485f4c8b480a957f7041f5397fedef9b28",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# List of ordinal variables with mappings\n# For all of these, the first values given are the highest and best\nord_cols = [\n\"ExterQual\",\n\"ExterCond\",\n\"BsmtQual\",\n\"BsmtCond\",\n\"BsmtExposure\",\n\"BsmtFinType1\",\n\"BsmtFinType2\",\n\"HeatingQC\",\n\"Electrical\",\n\"KitchenQual\",\n\"Functional\",\n\"FireplaceQu\",\n\"GarageQual\",\n\"GarageCond\",\n\"PoolQC\",\n\"Fence\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0dd49566-81b4-4e14-92f1-71b0952702db",
        "_uuid": "78db8e55b433625a7adad05c2ff51eadaf456e41",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "string_data = X[string_cols]\nprint(string_data.columns) #includes ordinal data\ncat_cols = []\n\nfor col in string_data:\n    if col not in ord_cols:\n        cat_cols.append(col)\n    \nprint(cat_cols)\nprint(type(cat_cols))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "878f4ebb-163f-493d-9972-63f690188b50",
        "_uuid": "b12e08da32c3892f29e27e3486f0d40961fb510b"
      },
      "cell_type": "markdown",
      "source": "## Step 8: Impute missing values"
    },
    {
      "metadata": {
        "_cell_guid": "58332f34-bbdd-46a9-9732-651cae09a486",
        "_uuid": "1461e398b8fb027cf5582fbf5a77d243bc169c9d"
      },
      "cell_type": "markdown",
      "source": "Before we start imputing values, we need to make sure that our testing set data isn't contaminating our imputed values, so we'll break the data in to training and testing sets now."
    },
    {
      "metadata": {
        "_cell_guid": "a28020a0-36cb-4c3b-a4cd-89645f005403",
        "collapsed": true,
        "_uuid": "f3b3dcc1c7b2a5a89fc4e6f747fe2eb5a71ab080",
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size = .3, random_state = 34)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "626e35fd-5c61-4e81-b929-ed1fd423ffda",
        "_uuid": "f99d6cce06fe9fb9cc112d149cbbf9ce18140307",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# impute missing values for string data (ordinal and categorical)\ns_imputer = CategoricalImputer()\n\nX_train_string = X_train_s[string_cols].values\nX_train_string = s_imputer.fit_transform(X_train_string)\nX_train_string = pd.DataFrame(X_train_string, columns = string_cols)\n\nprint(X_train_string.shape, X_train_string.head(3))\n\nX_test_string = X_test_s[string_cols].values\nX_test_string = s_imputer.transform(X_test_string)\nX_test_string = pd.DataFrame(X_test_string, columns = string_cols)\n\n# all categorical and ordinal missing data points have now been replaced with the most common value in each feature\n# still need to impute the values for the actual interval data. We'll do that with the most frequent value, too.\n# then we need to combine into one whole dataframe\n# first time I did this there were some cases where the most common case is not-existent, \n# so need to either delete those variables or somehow deal with them\n\nX_train_cat = X_train_string[cat_cols]\nX_test_cat = X_test_string[cat_cols]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e9dc194e-7d55-4652-aaa5-a043fd767c50",
        "_uuid": "47622d47ec72c759e36e277a1c7a36c934e9194c"
      },
      "cell_type": "markdown",
      "source": "### We're going to treat the ordinal data as interval data for now. Later we might build some models with the ordinal data as categorical data."
    },
    {
      "metadata": {
        "_cell_guid": "20dd3a44-6d6b-4279-87a6-60483ca3c047",
        "_uuid": "e36249cfd4697295308c8d15f1e94a3420f7d83c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# imputing missing values with most freqent values for the true interval columns\nn_imputer = Imputer(missing_values='NaN', copy = True, strategy = 'most_frequent')\n\nnumeric_cols.remove('SalePrice')\n\nX_train_num = X_train_s[numeric_cols]\nX_train_num = n_imputer.fit_transform(X_train_num)\nX_train_num = pd.DataFrame(X_train_num, columns = numeric_cols)\n\nX_test_num = X_test_s[numeric_cols]\nX_test_num = n_imputer.transform(X_test_num)\nX_test_num = pd.DataFrame(X_test_num, columns = numeric_cols)\n\nprint(X_test_num.head(3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fed65f59-a6be-482a-81a6-f40e6b8e7b6f",
        "_uuid": "85d4465fa154e2826d497bd1e5cf009af93db295",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# mapping string ordinal data \nord_cols_map = [\n    {\"col\":\"ExterQual\",    \n    \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.nan)\n    ]},\n    {\"col\":\"ExterCond\",\n    \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.nan)\n    ]}, \n    {\"col\":\"BsmtQual\",\n    \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Mn',2), \n        ('No',1), \n        ('NA',np.nan)   \n    ]},\n    {'col': \"BsmtCond\",\n    'mapping': [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA', np.NaN)\n    ]},\n    {'col': \"BsmtExposure\",\n    'mapping': [\n        ('Gd', 4),\n        ('Av', 3),\n        ('Mn', 2),\n        ('No', 1),\n        ('NA', np.NaN)\n    ]},\n    {'col': \"BsmtFinType1\",\n    'mapping': [\n        ('GLQ', 6),\n        ('ALQ', 5),\n        ('BLG', 4),\n        ('Rec', 3),\n        ('LwQ', 2),\n        ('Unf', 1),\n        ('NA', np.NaN)\n    ]},\n    {'col': \"BsmtFinType2\",\n    'mapping': [\n        ('GLQ', 6),\n        ('ALQ', 5),\n        ('BLG', 4),\n        ('Rec', 3),\n        ('LwQ', 2),\n        ('Unf', 1),\n        ('NA', np.NaN)\n    ]},\n     {\"col\":\"HeatingQC\",\n     \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n    ]}, \n    {\"col\": \"Electrical\",\n    \"mapping\":[\n        ('SBrkr', 4),\n        ('FuseA', 3),\n        ('FuseF', 2),       \n        ('FuseP', 1),\n    ]},\n    {\"col\":\"KitchenQual\",\n    \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n    ]}, \n    {'col':\"Functional\",\n     'mapping': [\n        ('Typ', 8),\n        ('Min1', 7),\n        ('Min2', 6),   \n        ('Mod', 5),\n        ('Maj1', 4),\n        ('Maj2', 3),\n        ('Sev', 2),\n        ('Sal', 1)\n     ]},\n    {\"col\":\"FireplaceQu\",\n    \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.NaN)\n    ]}, \n    {'col': \"GarageQual\",\n     \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.NaN)\n     ]},\n    {'col': \"GarageCond\",\n     \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.NaN)\n    ]},\n    {'col': \"PoolQC\",\n     \"mapping\": [\n        ('Ex',5), \n        ('Gd',4), \n        ('TA',3), \n        ('Fa',2), \n        ('Po',1), \n        ('NA',np.NaN)\n    ]},\n    {'col': \"Fence\", \n     \"mapping\": [\n        ('GdPrv', 5),\n        ('MnPrv', 4),\n        ('GdWo', 3), \n        ('MnWw', 2),\n        ('NA', np.NaN)\n    ]}]\n\n# use the Ordinal Encoder to map the ordinal data to interval and then fit transform \nencoder = ce.OrdinalEncoder( return_df = True, mapping = ord_cols_map)  #NaNs get -1\nX_train_ord = encoder.fit_transform(X_train_string)\nX_test_ord = encoder.transform(X_test_string)\n\n# ord has ordinal and categorical data in it!\nprint(X_train_ord.head(2))  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5e903181-3f82-464b-8e9c-fc57ae221e90",
        "_uuid": "b3f6a3aa47e4ab85f1af7deef8d120a870ce660c"
      },
      "cell_type": "markdown",
      "source": "Let's fit a model with our five numeric features and our ordinal encoded features encoded as interval data  and unencoded categorical data. "
    },
    {
      "metadata": {
        "_cell_guid": "3db79b79-fcd9-4655-8764-a9d57a8f9e31",
        "_uuid": "26e1901f62375d1ebeaa11ec1a3771e7af3e47e6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# X_train_ord is ordinal cols training X df and categorical \n# X_train_cat is categorical cols training X df\n# X_train_num  is numeric cols training X df\n\nX_train_ord.reset_index(drop=True)\nX_train_cat.reset_index(drop = True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f3368e77-4d83-4653-825f-3e8403f2755b",
        "_uuid": "5d6c91e9df04f4755b549311a3e0a68f9735adc7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X_train_ord.columns, X_train_num.columns)\n\nX_train_all = pd.concat([X_train_ord, X_train_num], axis = 1)\nprint(X_train_all.shape)\nprint(X_train_all.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1e754412-7eff-40d5-88d2-0710c28a3abc",
        "scrolled": false,
        "_uuid": "f8b101ccadb8081eedc850714e0c873bfd59398c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X_test_all = pd.concat([X_test_ord, X_test_num], axis = 1)\n\nprint(X_test_all.shape, X_test_all.head(2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "aa5f8e5a-5df2-4c73-991f-a38d295441d3",
        "_uuid": "38c691be3aa71f5497701ca14115be0bf57f2047"
      },
      "cell_type": "markdown",
      "source": "## Step 9: One hot encode the categorical data and make a model that includes it"
    },
    {
      "metadata": {
        "_cell_guid": "56ef78b0-81b0-476d-80df-588b97be9ba1",
        "_uuid": "483a0b9390338c9841e0bbb60a4c465cb0e7acb5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(cat_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3dc2dd7b-1ace-41df-be19-6b9c1665bfa4",
        "_uuid": "d1ffdef0af8534c7387fbe9dada8bb11e1cdd19f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X_train_all.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b50cc635-0b0b-49be-9921-e6e87ada551a",
        "_uuid": "5c799512b354db16fa4f4d8e2746becc83e1ef99",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# nominal categorical data gets one-hot-encoder treatment\n# could do with sklearn one hot endcoder\nprint(X_train_all.head(2))\n\n\nX_train = pd.get_dummies(data= X_train_all, prefix = \"Dummy_\", columns = cat_cols, drop_first= True)\nprint(X_train.shape)\nX_train = X_train.drop(cat_cols, axis = 1)\nprint(X_train.head(3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8024b81d-0ded-4bf7-908e-9fa5e7f16f59",
        "collapsed": true,
        "_uuid": "2d54558087b5161c7d9b6bf9e222b94cd0144baf",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# same treatment for test features\nX_test = pd.get_dummies(data= X_test_all, prefix = \"Dummy_\", columns = cat_cols, drop_first= True)\nX_test = X_test.drop(cat_cols, axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1c69cc05-7010-4056-bf13-11e752aa7d59",
        "_uuid": "12d82e055ecd93198eb89af35a2af052009bf25a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X_train.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ec6e5eb6-f94f-4f3d-a703-8c46b984fa0c",
        "_uuid": "f527c466d899062bdcf0de414c428c12ee17d2c7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n\n# because the test data doesn't have as many categories present it doesn't one hot encode as many dummy columns\n\n# Let's align the data frames - we only use the dummies in the training data to build our mode - \n# -we can't learn from the dummies only in the test data\n\nX_train, X_test = X_train.align(X_test, join = \"left\", fill_value = 0, axis = 1) \n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_train.columns)\nX_train.head(3)\n# there are still -1s ins ordinal data !!!!, thought those should have been filled above and 4 fences",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0c4266e2-3a7d-4d04-bf78-714695336dab",
        "_uuid": "067038479228609b00dc207eb4c48af52ea5254d"
      },
      "cell_type": "markdown",
      "source": "That's' a lot of columns, we'll look at trimming that down to the most important ones in a bit. Let's fit a linear regression model and now and see what we get."
    },
    {
      "metadata": {
        "_cell_guid": "2f379fdc-2fa2-4c9a-a77f-80b34d71b905",
        "_uuid": "488cb953ace2987a6f3aa7adcb2e3fd0c060df24",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# fit and run a linear regression model with all columns\n\nmod = LinearRegression()\nmod.fit(X_train, y_train_s)\npredictions = mod.predict(X_test)\n\nprint(predictions[:5])\n\nrmse = sqrt(mean_squared_error(y_test_s, predictions))\nprint(\"score: {0:,.0f}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8bec7934-5f61-47a0-acce-44a2431c97cd",
        "collapsed": true,
        "_uuid": "2c0078225a34868c3e64e7484fe6718251336f48"
      },
      "cell_type": "markdown",
      "source": "This looks wierd. Maybe too many features. The RMSE has gone up! All the work and we aren't doing better. Let's try just picking a few variables with Lasso."
    },
    {
      "metadata": {
        "_cell_guid": "16aba4a8-7300-4509-8ca5-161c5f34378b",
        "_uuid": "fcd4c539f585017f4a90e1bb956711d6d0ea1ce5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mod = Lasso()\nmod.fit(X_train, y_train_s)\npredictions = mod.predict(X_test)\n\nprint(predictions[:5])\n\nrmse = sqrt(mean_squared_error(y_test_s, predictions))\nprint(\"score: {0:,.0f}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7d501f4c-f9f7-4ca5-abfa-f56b667448c4",
        "_uuid": "7235f3203e543ec668de6e4fb001fcc7c5bf515e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mod = KNeighborsRegressor()\nmod.fit(X_train, y_train_s)\npredictions = mod.predict(X_test)\n\nprint(predictions[:5])\n\nrmse = sqrt(mean_squared_error(y_test_s, predictions))\nprint(\"score: {0:,.0f}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8e124cac-9ab3-4257-93ee-ea5717b8a390",
        "_uuid": "f7ddfd6ef0e51e3e2e6cf43fee2636791724eb17"
      },
      "cell_type": "markdown",
      "source": "That's getting worse. Ok, let's try a tree-based model."
    },
    {
      "metadata": {
        "_cell_guid": "f9a3959e-a3d9-4fec-9be0-aae201d86f3c",
        "_uuid": "50ab28826b831a26f79ebe2011b8257cc7212629",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mod = GradientBoostingRegressor()\nmod.fit(X_train, y_train_s)\npredictions = mod.predict(X_test)\n\nprint(predictions[:5])\n\nrmse = sqrt(mean_squared_error(y_test_s, predictions))\nprint(\"score: {0:,.0f}\".format(rmse))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4bd81d9a-3964-4cb3-a191-577f69c4511d",
        "_uuid": "6dd2dfbd153fd48026d71c6a11ae220f5428736d"
      },
      "cell_type": "markdown",
      "source": "That's the best yet, by a small amount. Hmm. Scaling the data should help regression models, but not likely to have much of an effect on tree-based models."
    },
    {
      "metadata": {
        "_cell_guid": "8b5cdf55-d40b-4796-810d-1b36c9bc4b83",
        "_uuid": "7735063586800690b02dee43d9646ae9c3d36783"
      },
      "cell_type": "markdown",
      "source": "## Step 10: Transform and scale\nImportant for some machine learning algorithms, including regression-based models, Kernel SVMs, Neural Nets. \n\n> Many machine learning algorithms in sklearn requires standardized data which means having zero mean and unit variance.\n> Standardization (or Z-score normalization) is the process where the features are rescaled so that they’ll have the properties of a standard normal distribution with μ=0 and σ=1, where μ is the mean (average) and σ is the standard deviation from the mean. \n\nSource: https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/\n\nWe'll log transform and scale the true interval variables to improve the model output.\n\nLog transform to improve lineariety first so it gets to use the full variance as part of the operation.\nThen normalize the data so all features are on a similar scale.\nThen add polynomials and interactions.\n\nWe'll have to break apart the data_types and then retransform them. Dummy encoded categorical data won't be affected here - we'll focus on the numerical and ordinal data.\n"
    },
    {
      "metadata": {
        "_cell_guid": "cc45182f-ad00-4c4b-9acd-547adb095741",
        "collapsed": true,
        "_uuid": "773304cae86a03262cca0045fa7787c4786b693a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# drop duplicate columns\nX_train = X_train.loc[:,~X_train.columns.duplicated()]\nX_test = X_test.loc[:,~X_test.columns.duplicated()]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "103ab71b-ead4-43d2-ac0d-0962ff3ea014",
        "_uuid": "86abea839335d51df0f6d105a77c78e38249ec8c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0f5382bb-cde5-4ab2-bb2e-8b1880afecdc",
        "collapsed": true,
        "_uuid": "a323129c25c9f6ffaa83b8b53861d1745e382635",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# making all true interval data positive for log transformation\nX_train[numeric_cols] += 2\nX_train[ord_cols] += 2\n\nX_test[numeric_cols] += 2\nX_test[ord_cols] += 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "24a5137c-a3a7-4509-aede-063bd3b036d0",
        "_uuid": "594447a3b48be7b26cc5db31e4d513e270280932",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# log transform the true interval variables to improve the model - not sure if we'll transform ordinal data - I think not\nprint(X_train.columns)\n#X_train[numeric_cols] = X_train[numeric_cols].apply(np.log)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "23218a16-e931-4200-98fd-23a0645246d1",
        "_uuid": "e46eed9ad303474d261bf98cc1b0a2ff7a592627",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "02466f84-f2f0-4b42-b1b3-0416382bc1d6",
        "_uuid": "f61f7478834e1ed5910496915b0ed84683709dc1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scale data\nscaler = StandardScaler()\nscaler.fit_transform(X_train)\n\nscaler.transform(X_test)\n\nprint(X_train.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "57d30d15-11a5-4ae7-a5ab-44b23cff4df7",
        "collapsed": true,
        "_uuid": "d1ed79567a096467628d4dc33d67168bfdbeca97",
        "trusted": false
      },
      "cell_type": "code",
      "source": "mod = GradientBoostingRegressor()\nmod.fit(X_train, y_train_s)\npredictions = mod.predict(X_test)\n\nprint(predictions[:5])\n\nrmse = sqrt(mean_squared_error(y_test_s, predictions))\nprint(\"score: {0:,.0f}\".format(rmse))\n\n\n# add polynomials/interactions\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b087f68a-bfda-4436-a00d-58187399d7ba",
        "_uuid": "c4d196209ca983d7034861e0adbd6e0c98961697"
      },
      "cell_type": "markdown",
      "source": "A slight improvement in our best score. Hopefully we aren't overfitting. Let's submit to kaggle and see how we do."
    },
    {
      "metadata": {
        "_cell_guid": "4103a5e4-99d4-4405-a85c-571a0fb5101e",
        "_uuid": "571bc41028bd1be25917389dfa8100f39cfd2acb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## submit to kagl",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "685b9d32-a58e-4056-a915-3b7bb9e43f72",
        "_uuid": "795227b682d3b079b536da723332b8661b4e2161"
      },
      "cell_type": "markdown",
      "source": "## Step 11: Bad outlier and abnormal data removal\n\n> Note that outliers are not necessarily \"bad\" data-points; indeed they may well be the most important, most information rich, part of the dataset. Under no circumstances should they be automatically removed from the dataset. Outliers may deserve special consideration: they may be the key to the phenomenon under study or the result of human blunders.\n\n[http://www.physics.csbsju.edu/stats/box2.html](http://www.physics.csbsju.edu/stats/box2.html)\n\nHowever, after consideration you may still want to remove the outliers. Our aims with a model are pragmatic - we want the model that will best generalize the best to new data. I suggest testing your model with and without the outliers excluded.\n\nObviously if the outliers are important to the phenomenon you are studying, don't get rid of them.\n\nIn this case, we'll exclude the following outliers.\n\n\nProbably get rid of abnormal sales, too.\n"
    },
    {
      "metadata": {
        "_cell_guid": "c482f9fb-68e6-43dd-87ef-438a9eb803fe",
        "_uuid": "3823d9f601c725ba9e7116b4c55224be569ed85f"
      },
      "cell_type": "markdown",
      "source": "## Step 11: Make a more sophisticated model building process with more advanced algorithms\nLet's try a few different models in a gridsearchCV  and, if too slow, randomsearchCV. \nEither make a pipeline or a number of functions to make switching out models easier.\nWe could try out LightGBH, which maybe the performance model likely to work best for regression tasks.\n"
    },
    {
      "metadata": {
        "_cell_guid": "d1ae2192-eb05-43b5-94b3-506c68fd71d5",
        "collapsed": true,
        "_uuid": "7f28a059356505089d6245ae0e8512156e8b5e20",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e04feddc-f50a-46a6-a1a7-225c44ae3555",
        "_uuid": "811210798314d934a5e7591e945c356d7580bfe0"
      },
      "cell_type": "markdown",
      "source": "## Step 12: Test against holdout data \nNot a bad idea to do this earlier, to make sure we aren't overfitting. We can submit a few models to the Kaggle competion and see how the RMSE changes."
    },
    {
      "metadata": {
        "_cell_guid": "e2be5404-319e-43ed-859f-a27381800d0b",
        "_uuid": "9ba9f39bcbb82f6e084bb0df411faf640df71bf1"
      },
      "cell_type": "markdown",
      "source": "## Step 13: Iterate\nEngineer more features -> Fit Models -> Score -> Submit"
    },
    {
      "metadata": {
        "_cell_guid": "0dc063a0-d62b-475e-8c46-b180054ab941",
        "_uuid": "b8904b9dcf27a2dc92930809387c7dddb45dc822"
      },
      "cell_type": "markdown",
      "source": "`## To do:\n\n### Time series\nWe've got a few dates in the data. More recent sales are likely to be more valuable to the model. We'll look at this later. \n\nOur google sheet shows that all the data-related variables are coded as integers.\n\n### Use KNN to better impute values\nuse this method:\n[https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637](https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637)\n\n### Binary encode the categorical variables\nSee how that performs compared to treading the categorical data as interval data"
    },
    {
      "metadata": {
        "_cell_guid": "58ef407f-3137-4552-8b8f-1a510ad75f29",
        "_uuid": "eeb4a16f56a6929cca94a719b947bd9068b3896f"
      },
      "cell_type": "markdown",
      "source": "# Lessons learned\nI should separate the interval, ordinal, and categorical data right away.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}